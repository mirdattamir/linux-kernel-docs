# Linux Storage Subsystem
## Full Architecture Documentation (Block Layer + Storage Drivers)

---

## 1Ô∏è‚É£ High-Level Purpose

### What Problem It Solves
The Storage Subsystem provides a **unified I/O pipeline** for all block-based storage devices. It handles:
- **I/O scheduling and merging** for optimal disk access patterns
- **Multi-queue architecture (blk-mq)** for parallelism on modern hardware
- **Request queueing and dispatching** to device drivers
- **Block device abstraction** for consistent interface to storage

### Position in System Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     VFS Layer (filesystems)             ‚îÇ
‚îÇ     Page cache, buffer heads            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ submit_bio()
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Block Layer (block/)                 ‚îÇ
‚îÇ  - BIO management                        ‚îÇ
‚îÇ  - Request queue (blk-mq)                ‚îÇ
‚îÇ  - I/O scheduling                        ‚îÇ
‚îÇ  - I/O accounting                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ queue_rq()
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Storage Drivers                        ‚îÇ
‚îÇ   - NVMe (drivers/nvme/)                 ‚îÇ
‚îÇ   - SCSI (drivers/scsi/)                 ‚îÇ
‚îÇ   - ATA (drivers/ata/)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ PCIe/DMA
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Hardware (SSD, HDD, RAID)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Interaction with Other Subsystems
- **VFS/Filesystems**: Receives I/O requests via `submit_bio()`
- **Memory Management**: Uses page allocator for bounce buffers, DMA mapping
- **Device Model**: Integrates with kernel device hierarchy (sysfs)
- **Power Management**: Handles device suspend/resume
- **Tracing**: Blktrace/ftrace integration for I/O observability

---

## 2Ô∏è‚É£ Directory Mapping

### Block Layer
```
block/
‚îú‚îÄ‚îÄ blk-core.c           # Core block layer initialization
‚îú‚îÄ‚îÄ blk-mq.c             # Multi-queue block layer (main logic)
‚îú‚îÄ‚îÄ blk-mq-tag.c         # Tag allocation for requests
‚îú‚îÄ‚îÄ blk-mq-sched.c       # I/O scheduler interface
‚îú‚îÄ‚îÄ blk-merge.c          # I/O request merging
‚îú‚îÄ‚îÄ blk-settings.c       # Queue limits and settings
‚îú‚îÄ‚îÄ blk-sysfs.c          # Sysfs interface (/sys/block/)
‚îú‚îÄ‚îÄ blk-timeout.c        # Request timeout handling
‚îú‚îÄ‚îÄ blk-flush.c          # Cache flush handling (barriers)
‚îú‚îÄ‚îÄ blk-integrity.c      # Data integrity (T10 DIF/DIX)
‚îú‚îÄ‚îÄ blk-crypto.c         # Inline encryption
‚îú‚îÄ‚îÄ bio.c                # BIO allocation and management
‚îú‚îÄ‚îÄ bfq-iosched.c        # BFQ I/O scheduler (Budget Fair Queueing)
‚îú‚îÄ‚îÄ mq-deadline.c        # mq-deadline I/O scheduler
‚îú‚îÄ‚îÄ kyber-iosched.c      # Kyber I/O scheduler
‚îú‚îÄ‚îÄ blk-cgroup.c         # Cgroup I/O control
‚îú‚îÄ‚îÄ blk-stat.c           # I/O statistics
‚îú‚îÄ‚îÄ blk-wbt.c            # Writeback throttling
‚îú‚îÄ‚îÄ blk-iocost.c         # I/O cost model (QoS)
‚îú‚îÄ‚îÄ blk-iolatency.c      # Latency-based QoS
‚îî‚îÄ‚îÄ partitions/          # Partition detection
```

### Storage Drivers
```
drivers/nvme/
‚îú‚îÄ‚îÄ host/
‚îÇ   ‚îú‚îÄ‚îÄ core.c           # NVMe core logic
‚îÇ   ‚îú‚îÄ‚îÄ pci.c            # NVMe PCIe driver
‚îÇ   ‚îú‚îÄ‚îÄ multipath.c      # NVMe multipathing
‚îÇ   ‚îú‚îÄ‚îÄ nvme.h           # NVMe data structures
‚îÇ   ‚îî‚îÄ‚îÄ ioctl.c          # NVMe ioctl interface
‚îî‚îÄ‚îÄ target/              # NVMe target (for creating NVMe devices)

drivers/scsi/
‚îú‚îÄ‚îÄ scsi.c               # SCSI mid-layer core
‚îú‚îÄ‚îÄ scsi_lib.c           # SCSI request queue management
‚îú‚îÄ‚îÄ scsi_error.c         # SCSI error handling
‚îú‚îÄ‚îÄ sd.c                 # SCSI disk driver
‚îú‚îÄ‚îÄ sg.c                 # SCSI generic driver
‚îú‚îÄ‚îÄ scsi_sysfs.c         # Sysfs integration
‚îî‚îÄ‚îÄ [hardware-specific]  # megaraid, mpt3sas, etc.

drivers/ata/
‚îú‚îÄ‚îÄ libata-core.c        # libata core
‚îú‚îÄ‚îÄ libata-scsi.c        # ATA-to-SCSI translation
‚îú‚îÄ‚îÄ libata-eh.c          # Error handling
‚îî‚îÄ‚îÄ ahci.c               # AHCI controller driver
```

---

## 3Ô∏è‚É£ Core Source Files

### Block Layer
| File | Purpose |
|------|---------|
| `block/bio.c` | **BIO management** - Allocate/free BIOs, bio_vec management |
| `block/blk-mq.c` | **Multi-queue core** - Request allocation, queue management, dispatch |
| `block/blk-mq-tag.c` | **Tag allocation** - Track outstanding requests (like IOMMU tags) |
| `block/blk-merge.c` | **I/O merging** - Merge adjacent requests for efficiency |
| `block/blk-flush.c` | **Cache flushing** - Handle FUA/PREFLUSH for data integrity |
| `block/bfq-iosched.c` | **BFQ scheduler** - Fair queueing for interactive workloads |
| `block/mq-deadline.c` | **Deadline scheduler** - Latency-focused scheduling |

### NVMe Driver
| File | Purpose |
|------|---------|
| `drivers/nvme/host/pci.c` | **PCIe driver** - PCI probe, queue setup, interrupt handling |
| `drivers/nvme/host/core.c` | **Core logic** - Namespace management, command submission |
| `drivers/nvme/host/multipath.c` | **Multipathing** - Path selection for redundant controllers |

### SCSI Layer
| File | Purpose |
|------|---------|
| `drivers/scsi/scsi_lib.c` | **Queue management** - SCSI command queueing |
| `drivers/scsi/sd.c` | **SCSI disk** - Block device interface for SCSI disks |
| `drivers/scsi/scsi_error.c` | **Error recovery** - Retry, reset, abort logic |

---

## 4Ô∏è‚É£ Core Data Structures

### struct bio (include/linux/blk_types.h)
**Purpose**: Represents a block I/O request at the page level

**Key Fields**:
```c
struct bio {
    struct bio              *bi_next;        // Next bio in chain
    struct block_device     *bi_bdev;        // Target block device
    unsigned int            bi_opf;          // Operation flags (READ/WRITE/FLUSH)
    unsigned short          bi_flags;        // Status flags
    unsigned short          bi_ioprio;       // I/O priority
    blk_status_t            bi_status;       // I/O status
    atomic_t                __bi_remaining;  // Reference count
    struct bvec_iter        bi_iter;         // Current position in bio_vec
    bio_end_io_t            *bi_end_io;      // Completion callback
    void                    *bi_private;     // Private data for callback
    unsigned short          bi_vcnt;         // Number of bio_vecs
    unsigned short          bi_max_vecs;     // Max bio_vecs allocated
    atomic_t                __bi_cnt;        // Reference count
    struct bio_vec          *bi_io_vec;      // Array of bio_vec segments
    struct bio_vec          bi_inline_vecs[]; // Inline storage for small I/O
};
```

**bio_vec (segment)**:
```c
struct bio_vec {
    struct page     *bv_page;       // Physical page
    unsigned int    bv_len;         // Length in bytes
    unsigned int    bv_offset;      // Offset within page
};
```

**Lifetime**: Allocated by filesystem/block layer, freed after I/O completion
**Ownership**: Submitted by upper layers, owned by block layer during processing
**Locking**: Reference counted (`__bi_cnt`), no explicit locks
**Memory Layout**: Inline bio_vecs for small I/O (typically 1-4 pages)

---

### struct request (include/linux/blk-mq.h)
**Purpose**: Represents a block device request (higher level than bio)

**Key Fields**:
```c
struct request {
    struct request_queue    *q;              // Associated queue
    struct blk_mq_ctx       *mq_ctx;         // Submission context (per-CPU)
    struct blk_mq_hw_ctx    *mq_hctx;        // Hardware queue context
    unsigned int            cmd_flags;       // Command flags
    req_flags_t             rq_flags;        // Request flags
    int                     tag;             // Unique tag for this request
    int                     internal_tag;    // Internal tag
    unsigned int            __data_len;      // Total data length
    sector_t                __sector;        // Starting sector
    struct bio              *bio;            // First bio in chain
    struct bio              *biotail;        // Last bio in chain
    struct list_head        queuelist;       // Link in various lists
    rq_end_io_fn            *end_io;         // Completion callback
    void                    *end_io_data;    // Private data
    ktime_t                 io_start_time_ns;// I/O start timestamp
    unsigned short          stats_sectors;   // Sectors for stats
    unsigned short          nr_phys_segments;// Physical segments (for DMA)
    unsigned short          ioprio;          // I/O priority
};
```

**Lifetime**: Allocated from blk-mq tag set, freed after completion
**Ownership**: Owned by block layer, passed to driver
**Locking**: Tag-based synchronization, no explicit locks
**Reference Counting**: Implicit via tag allocation

---

### struct request_queue (include/linux/blkdev.h)
**Purpose**: Per-device request queue (one per block device)

**Key Fields**:
```c
struct request_queue {
    struct request          *last_merge;     // Last merged request
    struct elevator_queue   *elevator;       // I/O scheduler
    struct blk_mq_tag_set   *tag_set;        // Tag set for requests
    struct queue_limits     limits;          // Queue limits (max sectors, etc.)
    unsigned int            nr_hw_queues;    // Number of hardware queues
    unsigned int            queue_depth;     // Max outstanding requests
    spinlock_t              queue_lock;      // Queue lock (legacy)
    struct kobject          kobj;            // Sysfs representation
    struct device           *dev;            // Associated device
    unsigned long           queue_flags;     // Queue flags
    atomic_t                mq_freeze_depth; // Freeze counter
    struct blk_mq_hw_ctx    **queue_hw_ctx;  // Array of hardware contexts
    unsigned int            nr_requests;     // Total requests available
};
```

**Lifetime**: Created during device initialization, destroyed on device removal
**Ownership**: Owned by block device
**Locking**: `queue_lock` for legacy single-queue, mostly lockless in blk-mq

---

### struct blk_mq_hw_ctx (block/blk-mq.h)
**Purpose**: Per-hardware-queue context (for parallel submission)

**Key Fields**:
```c
struct blk_mq_hw_ctx {
    struct {
        spinlock_t      lock;               // Protects dispatch list
        struct list_head dispatch;          // Dispatch list
    } ____cacheline_aligned_in_smp;

    unsigned long       state;              // Queue state
    struct request_queue *queue;            // Back pointer to queue
    unsigned int        queue_num;          // Hardware queue number
    unsigned int        nr_ctx;             // Number of software contexts
    struct blk_mq_ctx   **ctxs;             // Software contexts
    struct sbitmap      ctx_map;            // Bitmap of active contexts
    struct blk_mq_tags  *tags;              // Tag set
    struct blk_mq_tags  *sched_tags;        // Scheduler tags
    unsigned long       run_work;           // Work flags
    cpumask_t           cpumask;            // CPU affinity mask
    atomic_t            nr_active;          // Active requests
    struct hlist_node   cpuhp_online;       // CPU hotplug list
    struct delayed_work run_work;           // Async dispatch work
};
```

**Purpose**: Enables parallel I/O submission on multi-core systems
**Affinity**: Each hw_ctx typically mapped to specific CPUs for locality

---

### struct blk_mq_ctx (block/blk-mq.h)
**Purpose**: Per-CPU software context for request submission

**Key Fields**:
```c
struct blk_mq_ctx {
    struct {
        spinlock_t      lock;               // Protects request list
        struct list_head rq_lists[HCTX_MAX_TYPES]; // Request lists
    } ____cacheline_aligned_in_smp;

    unsigned int        cpu;                // CPU number
    unsigned short      index_hw[HCTX_MAX_TYPES]; // Hardware queue indices
    struct blk_mq_hw_ctx *hctxs[HCTX_MAX_TYPES];  // Hardware contexts
    struct request_queue *queue;            // Back pointer
    struct kobject      kobj;               // Sysfs representation
};
```

**Purpose**: Per-CPU submission to avoid lock contention

---

### NVMe Data Structures

### struct nvme_dev (drivers/nvme/host/pci.c)
**Purpose**: Represents an NVMe controller (PCIe device)

**Key Fields**:
```c
struct nvme_dev {
    struct nvme_ctrl    ctrl;               // Generic controller
    struct pci_dev      *pdev;              // PCIe device
    struct nvme_queue   *queues;            // I/O queues (Admin + I/O)
    struct dma_pool     *prp_page_pool;     // PRP (Physical Region Page) pool
    struct dma_pool     *prp_small_pool;    // Small PRP pool
    unsigned int        online_queues;      // Active queues
    unsigned int        max_qid;            // Max queue ID
    unsigned int        io_queues[HCTX_MAX_TYPES]; // I/O queue counts
    unsigned int        num_vecs;           // MSI-X vectors
    u32                 db_stride;          // Doorbell stride
    void __iomem        *bar;               // MMIO BAR
    unsigned long       bar_mapped_size;    // BAR size
    struct work_struct  remove_work;        // Device removal work
};
```

**Lifetime**: Created during PCIe probe, destroyed on removal
**Ownership**: Owned by NVMe driver
**Locking**: Per-queue locks for I/O submission

---

### struct nvme_queue (drivers/nvme/host/pci.c)
**Purpose**: NVMe submission/completion queue pair

**Key Fields**:
```c
struct nvme_queue {
    struct nvme_dev     *dev;               // Parent device
    spinlock_t          sq_lock;            // Submission queue lock
    struct nvme_command *sq_cmds;           // Submission queue commands
    struct nvme_completion *cqes;           // Completion queue entries
    dma_addr_t          sq_dma_addr;        // SQ DMA address
    dma_addr_t          cq_dma_addr;        // CQ DMA address
    u16                 q_depth;            // Queue depth
    u16                 cq_vector;          // MSI-X vector
    u16                 sq_tail;            // SQ tail pointer
    u16                 cq_head;            // CQ head pointer
    u16                 qid;                // Queue ID
    u8                  cq_phase;           // Phase bit
    unsigned long       flags;              // Queue flags
};
```

**DMA**: Uses DMA-coherent memory for queues
**Doorbells**: MMIO writes to notify device of new commands

---

## 5Ô∏è‚É£ Call Path Tracing

### Path 1: I/O Submission (Write)
```
Filesystem: ext4_file_write_iter()
              ‚Üì
iomap_file_buffered_write() [fs/iomap/buffered-io.c]
              ‚Üì
iomap_writepages() - Writeback daemon
              ‚Üì
submit_bio() [block/bio.c]
    ‚îú‚îÄ‚Üí bio_set_dev() - Set target block device
    ‚îú‚îÄ‚Üí generic_make_request()
    ‚îî‚îÄ‚Üí blk_mq_submit_bio() [block/blk-mq.c]
          ‚îú‚îÄ‚Üí blk_mq_get_request() - Allocate request from tag set
          ‚îú‚îÄ‚Üí blk_mq_bio_to_request() - Convert bio to request
          ‚îú‚îÄ‚Üí blk_mq_sched_insert_request() - Insert into scheduler
          ‚îÇ     ‚îî‚îÄ‚Üí mq_deadline_insert_requests() [if deadline scheduler]
          ‚îî‚îÄ‚Üí blk_mq_run_hw_queue() - Trigger dispatch
                ‚îú‚îÄ‚Üí __blk_mq_delay_run_hw_queue()
                ‚îî‚îÄ‚Üí __blk_mq_run_hw_queue()
                      ‚îî‚îÄ‚Üí blk_mq_sched_dispatch_requests()
                            ‚îú‚îÄ‚Üí blk_mq_do_dispatch_sched() - From scheduler
                            ‚îî‚îÄ‚Üí blk_mq_dispatch_rq_list()
                                  ‚îî‚îÄ‚Üí queue->mq_ops->queue_rq() - Driver callback
                                        ‚Üì
                                  nvme_queue_rq() [drivers/nvme/host/core.c]
                                        ‚îú‚îÄ‚Üí nvme_setup_cmd() - Build NVMe command
                                        ‚îú‚îÄ‚Üí nvme_submit_cmd() - Submit to SQ
                                        ‚îÇ     ‚îú‚îÄ‚Üí Write command to SQ
                                        ‚îÇ     ‚îú‚îÄ‚Üí Increment SQ tail
                                        ‚îÇ     ‚îî‚îÄ‚Üí writel(sq_tail, doorbell) - MMIO write
                                        ‚îî‚îÄ‚Üí Return BLK_STS_OK

Device Side: NVMe controller fetches command via DMA, executes I/O

Interrupt: MSI-X interrupt fires on completion
              ‚Üì
nvme_irq() [drivers/nvme/host/pci.c]
              ‚Üì
nvme_process_cq() - Process completion queue
    ‚îú‚îÄ‚Üí Read CQ entry
    ‚îú‚îÄ‚Üí Check phase bit
    ‚îú‚îÄ‚Üí Increment CQ head
    ‚îú‚îÄ‚Üí writel(cq_head, doorbell) - Acknowledge completion
    ‚îî‚îÄ‚Üí nvme_complete_rq()
          ‚îî‚îÄ‚Üí blk_mq_complete_request() [block/blk-mq.c]
                ‚îú‚îÄ‚Üí blk_mq_end_request()
                ‚îÇ     ‚îî‚îÄ‚Üí request->end_io() - Block layer callback
                ‚îÇ           ‚îî‚îÄ‚Üí bio->bi_end_io() - Filesystem callback
                ‚îî‚îÄ‚Üí blk_mq_free_request() - Return request to tag pool
```

### Path 2: Direct I/O (O_DIRECT)
```
User Space: pwrite(fd, buf, size, offset) with O_DIRECT
              ‚Üì
vfs_write() ‚Üí ext4_file_write_iter()
              ‚Üì
iomap_dio_rw() [fs/iomap/direct-io.c]
    ‚îú‚îÄ‚Üí get_user_pages() - Pin user pages
    ‚îú‚îÄ‚Üí iomap_dio_bio_iter() - Build BIO from user pages
    ‚îî‚îÄ‚Üí submit_bio_wait() - Submit and wait synchronously
          ‚Üì
[Same path as above through block layer and NVMe driver]
          ‚Üì
Wait for completion
          ‚Üì
unpin_user_pages() - Release pinned pages
          ‚Üì
Return to user space
```

### Path 3: Read with Page Cache Miss
```
sys_read() ‚Üí vfs_read() ‚Üí ext4_file_read_iter()
              ‚Üì
generic_file_buffered_read() [mm/filemap.c]
    ‚îú‚îÄ‚Üí find_get_page() - Check page cache ‚Üí MISS
    ‚îú‚îÄ‚Üí page_cache_alloc() - Allocate new page
    ‚îî‚îÄ‚Üí iomap_readahead() [fs/iomap/buffered-io.c]
          ‚îú‚îÄ‚Üí iomap_iter() - Get extent mapping
          ‚îî‚îÄ‚Üí iomap_readpage_iter()
                ‚îî‚îÄ‚Üí submit_bio() with READ operation
                      ‚Üì
[Same path through block layer and NVMe driver]
                      ‚Üì
Completion callback: iomap_read_end_io()
    ‚îú‚îÄ‚Üí SetPageUptodate(page)
    ‚îú‚îÄ‚Üí unlock_page(page)
    ‚îî‚îÄ‚Üí Wake up waiting readers

generic_file_buffered_read() resumes:
    ‚îî‚îÄ‚Üí copy_page_to_iter() - Copy to user space
```

### Path 4: Flush/FUA (Force Unit Access)
```
fsync(fd) ‚Üí vfs_fsync() ‚Üí ext4_sync_file()
              ‚Üì
filemap_write_and_wait_range() - Write dirty pages
              ‚Üì
blkdev_issue_flush() [block/blk-flush.c]
    ‚îú‚îÄ‚Üí bio_alloc() with REQ_OP_FLUSH | REQ_PREFLUSH
    ‚îî‚îÄ‚Üí submit_bio_wait()
          ‚Üì
blk_mq_make_request() [block/blk-mq.c]
    ‚îî‚îÄ‚Üí blk_insert_flush() [block/blk-flush.c]
          ‚îú‚îÄ‚Üí Queue flush request
          ‚îî‚îÄ‚Üí Dispatch to driver
                ‚Üì
nvme_queue_rq() with flush command
    ‚îî‚îÄ‚Üí nvme_setup_flush() - Build flush command
          ‚Üì
NVMe device flushes internal caches
          ‚Üì
Completion returns to filesystem
```

---

## 6Ô∏è‚É£ Concurrency Model

### Block Layer Locking

#### Per-CPU Software Context (blk_mq_ctx)
- **ctx->lock**: Spinlock protecting per-CPU request lists
- **Minimizes contention**: Each CPU has its own submission context

#### Hardware Queue Context (blk_mq_hw_ctx)
- **hctx->lock**: Spinlock protecting dispatch list
- **Lock-free fast path**: Tag-based request tracking
- **RCU**: Used for queue freeze/unfreeze operations

#### Tag Allocation
- **Sbitmap**: Lock-free bitmap for tag allocation
- **Per-hardware-queue tags**: Parallel tag allocation across queues
- **Wait queue**: Blocks when tags exhausted

### NVMe Driver Locking

#### Submission Queue Lock (nvme_queue->sq_lock)
- **Purpose**: Protects SQ tail pointer updates
- **Scope**: Per-queue (multiple queues for parallelism)
- **Held during**: Command submission, doorbell writes

#### Completion Queue Processing
- **IRQ context**: Runs in interrupt handler
- **No locks needed**: Single threaded per CQ
- **Softirq**: Can defer to NAPI-like polling

### Lock Ordering
```
1. queue_lock (legacy, rarely used in blk-mq)
2. ctx->lock (per-CPU context)
3. hctx->lock (hardware queue)
4. nvme_queue->sq_lock (driver)
```

### Atomic Operations
- **request->__bi_remaining**: Atomic reference count for split BIOs
- **request->__bi_cnt**: Atomic reference count
- **Tag bitmap**: Lock-free atomic bit operations

### RCU Usage
- **Queue freeze**: RCU-protected queue state transitions
- **Request completion**: RCU for request lifecycle

### Per-CPU Optimizations
- **blk_mq_ctx**: One per CPU to avoid contention
- **Statistics**: Per-CPU counters (disk_stats)
- **Tag caching**: Per-CPU tag cache for fast allocation

### Interrupt Context
- **nvme_irq()**: Runs in hardirq context
- **Completion processing**: Can run in softirq or threaded IRQ
- **Doorbell writes**: MMIO writes are serialized by hardware

---

## 7Ô∏è‚É£ Memory Model

### BIO Memory Management

**Allocation**:
- **bio_alloc()**: SLUB allocator (`fs_bio_set`)
- **bio_vec**: Inline for small I/O (<= 4 pages), separate allocation for larger
- **GFP flags**:
  - `GFP_NOIO` - During writeback (no recursion)
  - `GFP_KERNEL` - Normal allocation

**Memory Layout**:
```
struct bio (cacheline aligned)
    ‚Üì
bio_vec array (inline or separate)
    ‚Üì
Points to struct page (from page cache or user pages)
```

### Request Memory Management

**Allocation**:
- **blk_mq_alloc_request()**: From tag set pool
- **Pre-allocated**: Requests pre-allocated at queue init
- **Tag-based**: Request indexed by tag number

### DMA Mapping

**NVMe PRP (Physical Region Pages)**:
- **PRP lists**: Describe physical memory regions
- **DMA coherent**: Allocated with `dma_alloc_coherent()`
- **DMA pools**: Cached for performance (`prp_page_pool`)

**IOMMU**:
- **dma_map_page()**: Maps pages for device DMA
- **Scatter-gather**: Multiple physical regions in single I/O
- **Unmapping**: `dma_unmap_page()` after completion

### NUMA Awareness
- **Queue allocation**: Allocated on device's NUMA node
- **Request allocation**: From local node
- **Completion processing**: Processed on interrupt's CPU node

### Cacheline Alignment
- **struct request**: Cacheline aligned to avoid false sharing
- **blk_mq_hw_ctx**: Hot fields in first cacheline
- **nvme_queue**: Submission/completion pointers separated

### Memory Reclaim
- **Mempool**: Used for guaranteed allocation during memory pressure
- **GFP_NOIO**: Prevents recursive writeback during reclaim

---

## 8Ô∏è‚É£ Hardware Interaction

### PCIe Enumeration
```
PCI Bus Scan
    ‚Üì
nvme_probe() [drivers/nvme/host/pci.c]
    ‚îú‚îÄ‚Üí pci_enable_device() - Enable PCIe device
    ‚îú‚îÄ‚Üí pci_request_mem_regions() - Reserve BAR regions
    ‚îú‚îÄ‚Üí pci_set_master() - Enable bus mastering (DMA)
    ‚îú‚îÄ‚Üí dma_set_mask_and_coherent() - Set DMA addressing
    ‚îî‚îÄ‚Üí ioremap() - Map BAR to kernel virtual address
```

### NVMe MMIO Registers (BAR 0)
```
Offset 0x0000: Controller Capabilities (CAP)
Offset 0x0008: Version (VS)
Offset 0x0014: Controller Configuration (CC)
Offset 0x001C: Controller Status (CSTS)
Offset 0x0024: Admin Queue Attributes (AQA)
Offset 0x0028: Admin Submission Queue Base (ASQ)
Offset 0x0030: Admin Completion Queue Base (ACQ)
Offset 0x1000+: Doorbell Registers (SQ/CQ doorbells)
```

### Doorbell Mechanism
**Submission Doorbell** (MMIO write):
```c
writel(queue->sq_tail, queue->q_db);  // Notify device of new commands
```

**Completion Doorbell** (MMIO write after processing):
```c
writel(queue->cq_head, queue->q_db + queue->dev->db_stride); // Acknowledge completions
```

### DMA Operations

**Command Submission (Device reads)**:
1. CPU writes NVMe command to submission queue (DMA-coherent memory)
2. CPU writes doorbell register (MMIO)
3. Device DMA reads command from submission queue
4. Device fetches data using PRPs (DMA read from page cache)

**Data Transfer**:
- **Write**: Device DMA reads from host memory (PRPs point to pages)
- **Read**: Device DMA writes to host memory

**Completion Posting (Device writes)**:
1. Device DMA writes completion entry to completion queue
2. Device raises MSI-X interrupt
3. CPU DMA reads completion queue (check phase bit)
4. CPU writes completion doorbell (MMIO)

### Interrupt Handling

**MSI-X** (Message Signaled Interrupts Extended):
- **Multiple vectors**: One per I/O queue for parallelism
- **CPU affinity**: Interrupt routed to specific CPU
- **Registration**:
```c
pci_alloc_irq_vectors(pdev, 1, num_queues, PCI_IRQ_MSIX);
request_irq(irq, nvme_irq, IRQF_SHARED, "nvme", queue);
```

**IRQ Coalescing**:
- **Interrupt Coalescing**: NVMe feature to batch completions
- **Polling mode**: io_poll() for ultra-low latency

### Device Reset
```
Controller Failure Detection
    ‚Üì
nvme_reset_work() [drivers/nvme/host/pci.c]
    ‚îú‚îÄ‚Üí nvme_dev_disable() - Disable queues, stop I/O
    ‚îú‚îÄ‚Üí CC.EN = 0 (MMIO write) - Disable controller
    ‚îú‚îÄ‚Üí Wait for CSTS.RDY = 0
    ‚îú‚îÄ‚Üí CC.EN = 1 - Re-enable controller
    ‚îú‚îÄ‚Üí nvme_setup_io_queues() - Recreate queues
    ‚îî‚îÄ‚Üí nvme_start_queues() - Resume I/O
```

---

## 9Ô∏è‚É£ Performance Considerations

### Blk-MQ Parallelism
- **Multiple hardware queues**: One per CPU or NVMe queue
- **Lock-free fast path**: Tag-based tracking reduces contention
- **Per-CPU submission**: Avoids cross-CPU communication

### I/O Scheduling Tradeoffs
- **None (noop)**: No scheduling overhead (best for NVMe)
- **mq-deadline**: Latency guarantees for HDDs
- **BFQ**: Fair queueing for interactive workloads
- **Kyber**: Latency-based for mixed workloads

### Request Merging
- **Front merge**: Append to existing request start
- **Back merge**: Append to existing request end
- **Full merge**: Merge two requests
- **Tradeoff**: CPU overhead vs. I/O reduction

### Tag Depth Tuning
- **Large depth**: More parallelism, higher memory usage
- **Small depth**: Lower latency variance, less memory
- **NVMe typical**: 1024-4096 tags per queue

### NUMA Locality
- **Queue affinity**: Place queues on device's NUMA node
- **IRQ affinity**: Route interrupts to local CPUs
- **Memory allocation**: Allocate from local node

### Interrupt Mitigation
- **IRQ coalescing**: Batch completions to reduce interrupts
- **Polling mode (io_poll)**: CPU polls CQ instead of interrupts
  - **Tradeoff**: Lower latency but higher CPU usage

### Cacheline Optimization
- **Request alignment**: Avoid false sharing between CPUs
- **Read-mostly fields**: Separate from write-heavy fields
- **Completion processing**: Batch to improve cache reuse

### Direct I/O Benefits
- **Bypass page cache**: Eliminate memory copy
- **User-controlled**: Application manages buffering
- **Use case**: Databases with custom caching

### Writeback Throttling (WBT)
- **Purpose**: Prevent write saturation affecting reads
- **Mechanism**: Throttle writes based on read latency
- **Adaptive**: Adjusts to device characteristics

### I/O Cost Models (blk-iocost)
- **Purpose**: Fair sharing among cgroups
- **Mechanism**: Model device performance, assign costs
- **QoS**: Latency-sensitive vs. throughput-oriented

---

## üî∑ ASCII Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FILESYSTEM LAYER                             ‚îÇ
‚îÇ    ext4 / XFS / btrfs / F2FS / page cache / buffer heads       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ submit_bio()
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     BLOCK LAYER (block/)                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  struct bio (page-level I/O descriptor)                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - bi_opf (READ/WRITE/FLUSH)                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - bio_vec[] (page, offset, length)                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚îÇ blk_mq_submit_bio()                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  BLK-MQ (Multi-Queue Block Layer)                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  CPU 0         CPU 1         CPU 2         CPU 3         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ctx ‚îÇ      ‚îÇ ctx ‚îÇ      ‚îÇ ctx ‚îÇ      ‚îÇ ctx ‚îÇ         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                       ‚îÇ                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ   I/O Scheduler (optional)         ‚îÇ               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ   - BFQ / mq-deadline / kyber      ‚îÇ               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                       ‚îÇ                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ   Hardware Queue Context (hctx)    ‚îÇ               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ   HW Queue 0  HW Queue 1  ...      ‚îÇ               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                       ‚îÇ                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ   struct request (tag-based)       ‚îÇ               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ   - Bio list                       ‚îÇ               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ   - Tag ID                         ‚îÇ               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ                          ‚îÇ queue_rq()                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  STORAGE DRIVERS                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  NVMe Driver (drivers/nvme/host/)                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Queue 0  ‚îÇ  ‚îÇ Queue 1  ‚îÇ  ‚îÇ Queue 2  ‚îÇ  ‚îÇ Queue N  ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (Admin)  ‚îÇ  ‚îÇ  (I/O)   ‚îÇ  ‚îÇ  (I/O)   ‚îÇ  ‚îÇ  (I/O)   ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ SQ + CQ  ‚îÇ  ‚îÇ SQ + CQ  ‚îÇ  ‚îÇ SQ + CQ  ‚îÇ  ‚îÇ SQ + CQ  ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (DMA)    ‚îÇ  ‚îÇ (DMA)    ‚îÇ  ‚îÇ (DMA)    ‚îÇ  ‚îÇ (DMA)    ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                          ‚îÇ                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        ‚îÇ  Doorbell Writes (MMIO)            ‚îÇ             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        ‚îÇ  writel(sq_tail, doorbell_reg)     ‚îÇ             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ PCIe Bus
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     HARDWARE LAYER                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  NVMe SSD Controller                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ DMA Engine (reads SQ, writes CQ)                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Flash Translation Layer (FTL)                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ NAND Flash Array                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ MSI-X Interrupt Generation                             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Interrupt Flow (Completion)        ‚îÇ
         ‚îÇ                                     ‚îÇ
         ‚îÇ  1. Device writes CQ entry (DMA)   ‚îÇ
         ‚îÇ  2. Device raises MSI-X IRQ        ‚îÇ
         ‚îÇ  3. nvme_irq() processes CQ        ‚îÇ
         ‚îÇ  4. blk_mq_complete_request()      ‚îÇ
         ‚îÇ  5. bio_endio() callback           ‚îÇ
         ‚îÇ  6. Filesystem completion          ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üî∑ Call Graph: NVMe I/O Lifecycle

```
submit_bio(bio)
    ‚îÇ
    ‚îú‚îÄ‚Üí blk_mq_submit_bio()
    ‚îÇ     ‚îú‚îÄ‚Üí blk_mq_get_request() [allocate from tag pool]
    ‚îÇ     ‚îú‚îÄ‚Üí blk_init_request_from_bio() [convert bio ‚Üí request]
    ‚îÇ     ‚îú‚îÄ‚Üí blk_mq_sched_insert_request() [insert to scheduler]
    ‚îÇ     ‚îî‚îÄ‚Üí blk_mq_run_hw_queue() [trigger dispatch]
    ‚îÇ           ‚îî‚îÄ‚Üí blk_mq_dispatch_rq_list()
    ‚îÇ                 ‚îî‚îÄ‚Üí nvme_queue_rq() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                               ‚îÇ
    ‚îÇ                                               ‚ñº
    ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                              ‚îÇ  NVMe Driver (queue_rq)    ‚îÇ
    ‚îÇ                              ‚îÇ  1. nvme_setup_cmd()        ‚îÇ
    ‚îÇ                              ‚îÇ  2. nvme_map_data()         ‚îÇ
    ‚îÇ                              ‚îÇ  3. Write cmd to SQ         ‚îÇ
    ‚îÇ                              ‚îÇ  4. writel(doorbell)        ‚îÇ
    ‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                                               ‚îÇ
    ‚îÇ                                               ‚ñº
    ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                              ‚îÇ  NVMe Device               ‚îÇ
    ‚îÇ                              ‚îÇ  1. DMA read SQ            ‚îÇ
    ‚îÇ                              ‚îÇ  2. Execute I/O            ‚îÇ
    ‚îÇ                              ‚îÇ  3. DMA write CQ           ‚îÇ
    ‚îÇ                              ‚îÇ  4. Raise MSI-X IRQ        ‚îÇ
    ‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                                               ‚îÇ
    ‚îÇ                                               ‚ñº
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> nvme_irq() [interrupt handler]
                                              ‚îÇ
                                              ‚îú‚îÄ‚Üí nvme_process_cq()
                                              ‚îú‚îÄ‚Üí nvme_complete_rq()
                                              ‚îî‚îÄ‚Üí blk_mq_complete_request()
                                                    ‚îî‚îÄ‚Üí blk_mq_end_request()
                                                          ‚îî‚îÄ‚Üí bio_endio()
                                                                ‚îî‚îÄ‚Üí bio->bi_end_io()
                                                                      ‚îî‚îÄ‚Üí [Filesystem callback]
                                                                            ‚îî‚îÄ‚Üí Wake up waiters
                                                                                  ‚îî‚îÄ‚Üí I/O complete
```

---

**END OF STORAGE SUBSYSTEM DOCUMENTATION**
